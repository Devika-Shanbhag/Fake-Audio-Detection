{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASVspoof Fake audio detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AnWDiawtSLJ",
        "colab_type": "text"
      },
      "source": [
        "## Contents\n",
        "\n",
        "##### 1. Problem statement\n",
        "##### 2. Detailed description of data\n",
        "##### 3. Description of attacks and models used for detection\n",
        "##### 4. Baseline and SOTA architecture\n",
        "##### 5. Metrics\n",
        "##### 6. References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egCTzE2bGVvM",
        "colab_type": "text"
      },
      "source": [
        "## Link to project proposal\n",
        "https://docs.google.com/document/d/1K46pcbYzvXcz3di70oCOtl7iO4J0LP3QNHHRxRG-8Cg/edit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2srmtGLdvv7",
        "colab_type": "text"
      },
      "source": [
        "## 1. Problem statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN2yoIJZd1VS",
        "colab_type": "text"
      },
      "source": [
        "Deepfakes are artificial media in which audio, image, or video is fabricated and made to look like original/authentic content that represents humans. The problem at hand is **Detection of fake audio. Particularly, spoofing attacks on automatic speaker verification systems**. \n",
        "\n",
        "We aim to analyze & attempt to improve current approaches that combine deep learning and traditional machine learning architectures in an ensemble to build robust detection systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U05bNJ4pgJgv",
        "colab_type": "text"
      },
      "source": [
        "## 2. Data Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5pj0FhwiYH_",
        "colab_type": "text"
      },
      "source": [
        "[Research paper on ASVspoof 2019 Data analysis](https://arxiv.org/pdf/1911.01601.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXw2B66Ournf",
        "colab_type": "text"
      },
      "source": [
        "### Overview:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFR_MUyEu1WQ",
        "colab_type": "text"
      },
      "source": [
        "Challenge deals with two types of attacks Logical access(LA) and Physical access(PA)\n",
        "\n",
        "Sampling rate was 96 kHz which was downsampled to 16kHz\n",
        "107 speakers (46 male, 61 female)\n",
        "\n",
        "**LA**: Attack in feature manipulation. 7 Text to Speech and 6 Voice conversion spoofing systems\n",
        "\n",
        "Number of utterances\n",
        "Train: bonafide 2,580 | spoofed 22,800\n",
        "Development: bonafide 2,548 | spoofed 22,296\n",
        "Evaluation : 80,000\n",
        "\n",
        "**PA** : Attack at the sensor or physical environment. 9 attack types and 27 environments\n",
        "\n",
        "Number of utterances\n",
        "Train: bonafide 5,400 | spoofed 48,600\n",
        "Development: bonafide 5,400 | spoofed 24,300\n",
        "Evaluation : 135,000\n",
        "\n",
        "Types of features used in dataset creation :  \n",
        "60-dimensional Mel-cepstrum, 1-dimensional F0, 5-dimensional aperiodic component,  25 dimensional bandaperiodicity coefficients (BAPs) etc.\n",
        "\n",
        "Types of features that can be extracted from dataset :\n",
        "512-dimensional x-vector using Kaldi, Baseline model using CQCC features , Baseline model using LFCC features. Human evaluation was carried to obtain 27,000 bonafide scores of target speakers, 12,000 bona-fide scores for non-target\n",
        "speakers, and around 1,200 scores for spoofed utterances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1Wim5fRuvPI",
        "colab_type": "text"
      },
      "source": [
        "### Database structure:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTj_p46OuSis",
        "colab_type": "text"
      },
      "source": [
        "Some details about the database:\n",
        "\n",
        "1. Training and development data for the LA scenario are included in 'ASVspoof2019_LA_train'  ' ASVspoof2019_LA_dev'. Training dataset contains audio files with known ground-truth, which can be used to train systems to distinguish genuine and spoofed speech. The development dataset contains audio files with known ground-truth which can be used for the development of spoofing detection algorithms. Likewise, training and development data for the PA scenario are included in 'ASVspoof2019_PA_train'  ' ASVspoof2019_PA_dev'. \n",
        "\n",
        "2. Evaluation data for LA and PA are available in 'ASVspoof2019_LA_eval'  and 'ASVspoof2019_PA_eval', respectively.\n",
        "\n",
        "3. Dev and eval enrollment data for ASV are available in 'ASVspoof2019_{LA,PA}_dev' and 'ASVspoof2019_{LA,PA}_eval', respectively.\n",
        "\n",
        "4. Protocol and keys are available in 'ASVspoof2019_LA_{cm,asv}_protocols'  and ASVspoof2019_PA_{cm,asv}_protocols, respectively.\n",
        "\n",
        "5. Additional README.LA.txt files and README.pA.txt are included in packages. They are the extended version of ASVspoof2019_{LA,PA}_instructions_v1.txt originally used for the challenge to explain the database.\n",
        "\n",
        "6. The baseline results based on LFCC and CQCC can be reproduced using publicly released Matlab-based implementation of a replay attack spoofing detector http://www.asvspoof.org/asvspoof2019/ASVspoof_2019_baseline_CM_v1.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzt7W4tFgS44",
        "colab_type": "text"
      },
      "source": [
        "## 3. Description of attacks and models used for detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIq-9TtavFOM",
        "colab_type": "text"
      },
      "source": [
        "The ASVspoof challenge aims to encourage further progress through\n",
        "\n",
        "*   the collection and distribution of a standard dataset with varying spoofing attacks implemented with multiple, diverse algorithms\n",
        "*   a series of competitive evaluations for automatic speaker verification.\n",
        "\n",
        "#### Background:\n",
        "\n",
        "The ASVspoof 2019 challenge follows on from three special sessions on spoofing and countermeasures for automatic speaker verification held during INTERSPEECH 2013 [1], 2015 [2], and 2017 [3]. While the first edition in 2013 was targeted mainly at increasing awareness of the spoofing problem, the 2015 edition included the first challenge on the topic, accompanied by commonly defined evaluation data, metrics and protocols. The task in ASVspoof 2015 was to design countermeasure solutions capable of discriminating between bona fide (genuine) speech and spoofed speech produced using either text-to-speech (TTS) or voice conversion (VC) systems. The ASVspoof 2017 challenge focused on the design of countermeasures aimed at detecting replay spoofing attacks that could, in principle, be implemented by anyone using common consumer-grade devices.\n",
        "\n",
        "The ASVspoof 2019 challenge extends the previous challenge in several directions. The 2019 edition is the first to focus on countermeasures for all three major attack types, namely those stemming from TTS, VC and replay spoofing attacks. Advances with regards to the 2015 edition include the addition of up-to-date TTS and VC systems that draw upon substantial progress made in both fields during the last four years. ASVspoof 2019 thus aims to determine whether the advances in TTS and VC technology post a greater threat to automatic speaker verification and the reliability of spoofing countermeasures.\n",
        "\n",
        "Advances with regards to the 2017 edition concern the use of a far more controlled evaluation setup for the assessment of replay spoofing countermeasures. Whereas the 2017 challenge was created from the recordings of real replayed spoofing attacks, the use of an uncontrolled setup made results somewhat difficult to analyse. A controlled setup, in the form of replay attacks simulated using a range of real replay devices and carefully controlled acoustic conditions is adopted in ASVspoof 2019 with the aim of bringing new insights into the replay spoofing problem.\n",
        "\n",
        "#### **ASVspoof 2019**\n",
        "\n",
        "The 2019 edition aligns ASVspoof more closely with the field of automatic speaker verification. Whereas the 2015 and 2017 editions focused on the development and assessment of stand-alone countermeasures, ASVspoof 2019 adopts for the first time a new ASV-centric metric in the form of the tandem decision cost function (t-DCF) [4].\n",
        "\n",
        "The ASVspoof 2019 database encompasses two partitions for the assessment of logical access (LA) and physical access (PA) scenarios. Both are derived from the VCTK base corpus [5] which includes speech data captured from 107 speakers (46 males, 61 females). Both LA and PA databases are themselves partitioned into three datasets, namely training, development and evaluation which comprise the speech from 20 (8 male, 12 female), 10 (4 male, 6 female) and 48 (21 male, 27 female) speakers respectively. The three partitions are disjoint in terms of speakers, and the recording conditions for all source data are identical. While the training and development sets contain spoofing attacks generated with the same algorithms/conditions (designated as known attacks), the evaluation set also contains attacks generated with different algorithms/conditions (designated as unknown attacks). Reliable spoofing detection performance therefore calls for systems that generalise well to previously-unseen spoofing attacks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHgPIvwdwUku",
        "colab_type": "text"
      },
      "source": [
        "## 4. SOTA and Baseline models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B58efYZwx9ua",
        "colab_type": "text"
      },
      "source": [
        "### Criteria for judgement:-\n",
        "\n",
        "ASVspoof 2019 focuses on assessment of tandem systems consisting of both a spoofing countermeasure (CM) (designed by the participant) and an ASV system (provided by the organisers). The performance of the two combined systems is evaluated via the minimum normalized tandem detection cost function (t-DCF)\n",
        "The EER serves as a secondary metric. The EER corresponds to a CM operating point with equal miss and false alarm rates and was the primary metric for previous editions of ASVspoof.\n",
        "\n",
        "\n",
        "### Challenge Results:-\n",
        "\n",
        "The top-performing system for the LA scenario, T05, achieved a t-DCF of 0.0069 and EER of 0.22%. The top-performing system for the PA scenario, T28, achieved a t-DCF of 0.0096 and EER of 0.39%. Monotonic increases in the tDCF that are not always mirrored by monotonic increases in the EER show the importance of considering the performance of the ASV and CM systems in tandem.\n",
        "\n",
        "\n",
        "### Approaches used by different teams-\n",
        "\n",
        "#### Ensemble technique\n",
        "https://arxiv.org/pdf/1904.04589.pdf\n",
        "\n",
        "They train five deep models using raw audio or time frequency representations as input to minimise a binary crossentropy (CE) loss with an Adam optimiser and early stopping with a patience of P epochs. As the dataset has more spoofed examples, they replicate the bonafide examples to ensure each batch contains an equal number of bonafide and spoofed examples, which helps stabilise the training. At inference time, they use the output layer sigmoid activation as a score. They have provided model-specific training details below-\n",
        "\n",
        "#### CNN\n",
        "They use an utterance-level mean-variance normalized log spectrogram3 , computed using a 1024-point FFT with a hop size of 160 samples, as the input. For each task, we train two such CNN models, model A and B, on the first and last 4 seconds of each audio sample\n",
        "\n",
        "#### CRNN\n",
        "As input, they use a mean-variance (computed on train tr set) normalized log-Mel spectrogram of 40 Mel bands, computed on the first 5 seconds of truncated or looped audio samples, using a 1024-point FFT with a hop size of 256 samples. \n",
        "\n",
        "#### IDCNN\n",
        "In total, the model consists of 9 ReSE-2 blocks [22]. These blocks are a combination of ResNets [23] and SENets [24]. They use the multi-level feature aggregation, where the outputs of the last three blocks are concatenated and followed by a fully connected layer of 1024 units, batch normalization and ReLU layers, a 50% dropout layer and a fully connected layer of 1 unit with sigmoid activation. Each convolutional layer has filters of size 3, L2 weight regularizer of 0.0005 and all strides are of unit value. The raw audio input is 3.7 seconds in duration and randomly sampled segments of this size are selected from the recordings. \n",
        "\n",
        "#### Wave U-Net\n",
        "They use a modified version of the Wave-U-Net [25], with five layers of stride four, and without upsampling blocks (model E). The outputs of the last convolution are max-pooled across time, reducing the parameter count and incorporating the intuition that the important features in the tasks are temporally local. Finally, they apply a fully connected layer with a single output to yield a classification probability. They train the model using a batch size of 64, a learning rate of 10−5 and early stopping patience of P = 10 for both the LA and PA tasks, where an epoch is defined as 500 update steps\n",
        "\n",
        "#### Gaussian Mixture models\n",
        "They train three GMM models using 60-dimensional static, delta and acceleration (SDA) mel frequency cepstral coefficients (MFCCs) (model F), inverted mel frequency cepstral coefficients (IMFCCs)(model G), and sub-band centroid magnitude coefficients (SCMC) (model H), due to their performance on the ASVspoof 2015 and 2017 spoofing datasets\n",
        "\n",
        "#### Support Vector Machines\n",
        "They train two SVMs using i-vectors (model I) and the longterm-average-spectrum (LTAS) feature (model J) since they have shown good performance on prior spoofing datasets.\n",
        "\n",
        "\n",
        "#### RESULTS-\n",
        "\n",
        "This team achieve good performance on the PA and 3rd ranking on the LA tasks of the challenge. The PA task seems generally more difficult and should thus be the primary focus of future work.\n",
        "\n",
        "\n",
        "\n",
        "### Using VGG net and SincNet\n",
        "https://omilia.com/wp-content/uploads/2019/09/BUT_Omilia_anti_spoof_2019_system_description-2.pdf\n",
        "\n",
        "For this challenge, two different topologies were used for Physical access. The first one is a modified version of a VGG network  which has shown good performance in Audio Tagging and Audio Scene Classification. The second network is a modified version of a Light CNN (LCCN) which had the best performance for ASVSpoof2017 challenge. We have used a modified version of both networks for acoustic scene classification challenge 2019. In the following two sections, both networks will be explained in more detail-\n",
        "\n",
        "The VGG network comprises several convolutional and pooling layers followed by a statistics pooling and several dense layers which perform classification.  There are 6 convolutional blocks in the model, each containing 2 convolutional layers and one max-pooling. Each max-pooling layer reduce the size of frequency axis to half while only one of them reduces the temporal resolution.\n",
        "\n",
        "The LCNN network is a combination of convolutional and max-pooling layers and uses Max-Feature-Map (MFM) as non-linearity. MFM is a layer which simply reduce the number of output channels to the half by taking the maximum of two consecutive channels (or any other combination of two channels).\n",
        "\n",
        "\n",
        "#### RESULTS-\n",
        "\n",
        "For PA, they follwed the VGG architecture and obtained very competitive results in both development and evaluation sets, by fusing only two networks. For LA, they fused a VGG architecture with the recently proposed SincNet. The rationale for employing the latter was its ability to jointly optimize the networks and the feature extractor, which was shown to be very effective for speech and speaker recognition. Despite their efforts to prevent overfitting (mainly via attack-level cross validation in training and development), the results on LA showed the difficulty of the SincNet in generalizing to certain attacks which were significantly different to those in the training. They concluded that more research is required in order to make full use of end-to-end anti-spoofing architectures such as SincNet in cases of large mismatch between training and evaluation attacks.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Odd8bZY-opS",
        "colab_type": "text"
      },
      "source": [
        "## 5. ASVspoof metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyNo_dJo7XHP",
        "colab_type": "text"
      },
      "source": [
        "ASVspoof challenge uses two metrics: t-DCF (tandem detection cost function) and EER (equal error rate). Here we try to explain what these metrics are measuring briefly (reference: https://www.isca-speech.org/archive/Odyssey_2018/pdfs/68.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl6MdhdE_BJ-",
        "colab_type": "text"
      },
      "source": [
        "### Sub-systems in ASVspoof\n",
        "\n",
        "In ASVspoof challenge, we consider two different systems: <br>\n",
        "\n",
        "1) ASV (automatic speaker verification): This system is concerned with verifying whether the speakers in the given audios are same or different\n",
        "\n",
        "2) CM (counter measure): This is concerned with identifying whether a audio is spoofed or not\n",
        "\n",
        "Because the two sub-systems are inter-connected and affect each other's performance, we want a metric that measures the joint performance of the two sub-systems. This is where t-DCF comes into play.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r2g3Pz5_G5s",
        "colab_type": "text"
      },
      "source": [
        "### Dataset types\n",
        "Consider that for a given speaker we have three separate datasets: <br>\n",
        "\n",
        "a) target: audio of the speaker <br>\n",
        "b) non-target: audio of a different speaker <br>\n",
        "c) spoofed: spoofed audio of speaker <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W9SX_-q_LeH",
        "colab_type": "text"
      },
      "source": [
        "### Detection error rates\n",
        "Some detection error rates that we will use later: <br>\n",
        "\n",
        "* ASV\n",
        "     * Miss error rate: $P_{miss}^{asv}(t) = \\frac{1}{N_{tar}} \\sum_{i \\in S_{tar}} \\mathbb{I} \\{r_i \\leq t\\}$ <br>\n",
        "       This tries to estimate the probability of ASV system misclassifying a speaker audio as non-speaker audio/missing speaker audio (if the score of the model $r_i$ is $\\leq$ to a threshold $t$, the ASV system classifies the audio as non-speaker). $\\mathbb{I}$ function calculates the number of occurrences for which the condition holds true.\n",
        "     * False acceptance rate: $P_{fa}^{asv}(t) = \\frac{1}{N_{nontar}} \\sum_{i \\in S_{nontar}} \\mathbb{I} \\{r_i > t\\}$ <br>\n",
        "      This estimates the probability of a sample not belonging to speaker model but being falsely accepted as such by the ASV system\n",
        "      \n",
        "     * Because we can consider spoofs to be very close to speaker model, we can also define a quantity \"spoofs that were not missed by ASV\": $1 - P_{miss,spoof}^{asv}(t) = 1 - \\frac{1}{N_{spoof}} \\sum_{i \\in S_{spoof}} \\mathbb{I} \\{r_i \\leq t\\}$ which is 1 - probability that spoofs were *correctly* missed by the ASV system.\n",
        "\n",
        "* CM <br>\n",
        "    Because CM systems are not concerned with speaker verification, we pool the target and non-target audios into one common \"human\" audio dataset for measuring CM related metrics\n",
        "    * Miss error rate: $P_{miss}^{cm}(s) = \\frac{1}{N_{human}} \\sum_{j \\in S_{human}} \\mathbb{I} \\{q_j \\leq s\\}$.<br> This calculates the probability of a bonafide audio being incorrectly missed/classified as spoof by the CM system. Here, if $q_j$ is $\\leq$ some threshold $s$, an audio is classified as spoof.\n",
        "    * False acceptance rate: $P_{fa}^{cm}(s) = \\frac{1}{N_{spoof}} \\sum_{j \\in S_{spoof}} \\mathbb{I} \\{q_j > s\\}$.<br> This estimates the probability of a spoofed audio being falsely accepted as bonafide by the CM system\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0lvG4jV_Pu-",
        "colab_type": "text"
      },
      "source": [
        "### Equal error rate\n",
        "EER (equal error rate) is achieved when $P_{miss}^{system} = P_{fa}^{system}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIaEkfwk_SyL",
        "colab_type": "text"
      },
      "source": [
        "### Types of action outputs of sub-systems\n",
        "Now, consider ASV and CM systems working together. Each of the systems can take one action from $\\mathcal{A} = \\{ACCEPT, REJECT, SLEEP\\}$ ACCEPT and REJECT are what one might expect. ACCEPT indicates that the system passed the audio and REJECT indicates that the system rejected the audio. SLEEP comes into picture when the two systems run sequentially. If the first system itself rejects the audio, we consider the output of the second system to be SLEEP\n",
        "\n",
        "Considering this, the possible set of outcomes for the combined system will be: <br>\n",
        "$\\alpha_{1} = (ACCEPT, REJECT)$ <br>\n",
        "$\\alpha_{2} = (ACCEPT, ACCEPT)$ <br>\n",
        "$\\alpha_{3} = (REJECT, REJECT)$ <br>\n",
        "$\\alpha_{4} = (REJECT, ACCEPT)$ <br>\n",
        "$\\alpha_{5} = (REJECT, SLEEP)$ <br>\n",
        "$\\alpha_{6} = (SLEEP, REJECT)$ <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhz5CGws_Xwz",
        "colab_type": "text"
      },
      "source": [
        "### Detection cost function\n",
        "For a particular action, we define DCF (detection cost function) as: <br>\n",
        "$DCF(\\alpha_j) = \\sum_{i=1}^{M}\\pi_{i}*C(\\alpha_j|\\theta_i)*P_{err}(\\alpha_{j}|\\theta_{i})$ <br>\n",
        "Here, <br>\n",
        "1) $\\alpha_j$ corresponds to a particular action sequence <br>\n",
        "2) $\\sum_{i=1}^{M}$ corresponds to summing over individual actions in the correct sequence of actions <br>\n",
        "3) $\\pi_{i}$ corresponds to probability of a true action occurring <br>\n",
        "4) $C(\\alpha_{j}|\\theta_{i})$ is the cost associated with taking action $\\alpha_{j}$ when the true action is $\\theta_{i}$ <br>\n",
        "5) $P_{err}(\\alpha_{j}|\\theta_{i})$ is the probability that we will take action $\\alpha_{j}$ given that the true action is $\\theta_{i}$. For our case, these are the probabilities that we defined above.\n",
        "\n",
        "Then, $DCF = \\sum_{j=1}^{L}DCF(\\alpha_{j})$ where we are summing over all the actions we took"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPpC16V5_b-V",
        "colab_type": "text"
      },
      "source": [
        "### Tandem Detection Cost Function\n",
        "Now, for t-DCF.\n",
        "\n",
        "t-DCF$(s, t) = C_{miss}^{asv}*\\pi_{tar}*P_{a}(s,t) + C_{fa}^{asv}*\\pi_{nontar}*P_{b}(s,t) + C_{fa}^{cm}*\\pi_{spoof}*P_{c}(s,t) + C_{miss}^{cm}*\\pi_{tar}*P_{d}(s)$\n",
        "\n",
        "Here, <br>\n",
        "\n",
        "1) s is the threshold for CM system and t is the threshold for ASV system (as used in the error rates above) <br>\n",
        "2) $C_{miss}^{asv}=$ cost associated with ASV missing speaker audio; $C_{fa}^{asv}=$ cost associated with ASV falsely accepting non-target audio; $C_{fa}^{cm}=$ cost associated with CM system falsely accepting spoofed audio; and $C_{miss}^{cm}=$ cost associated with CM system falsely rejecting non-spoof audio <br>\n",
        "3) $\\pi_{tar}$ is probability of audio being from target dataset; $\\pi_{nontar}$ is probability of audio being from nontarget dataset; and $\\pi_{spoof}$ is probability of audio being from spoofed dataset\n",
        "\n",
        "For the last probability terms, in the equation: <br>\n",
        "\n",
        "a) $P_{a}(s,t)=(1-P_{miss}^{cm}(s))*P_{miss}^{asv}(t)$ that is probability of CM not missing human speech but ASV misclassifying.<br>\n",
        "b) $P_{b}(s,t)=(1-P_{miss}^{cm}(s))*P_{fa}^{asv}(t)$ that is the probability that CM does not miss human speech (non-target one in this case) but ASV falsely accepts non-target. <br>\n",
        "c) $P_{c}(s,t)=P_{fa}^{cm}(s)*(1-P_{miss,spoof}^{asv}(t))$ is the probability that CM falsely accepts spoofed audio and ASV does not miss spoofed audio <br>\n",
        "d) $P_{d}(s)=P_{miss}^{cm}(s)$ is the probability that CM falsely rejects target utterance thus not giving ASV a chance to classify/making ASV's prediction irrelevant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obbf8eAZxNmd",
        "colab_type": "text"
      },
      "source": [
        "## ASVSpoof evaluation plan: \n",
        "http://www.asvspoof.org/asvspoof2019/asvspoof2019_evaluation_plan.pdf\n",
        "\n",
        "## Paper mentioning about ASVspoof 2019 results\n",
        "https://arxiv.org/pdf/1904.05441.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y23e-SbSFUHC",
        "colab_type": "text"
      },
      "source": [
        "## 6. References\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "defguigbwuZd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "[1] Nicholas Evans, Tomi Kinnunen and Junichi Yamagishi, \"Spoofing and countermeasures for automatic speaker verification\", Interspeech 2013,  925-929, August 2013\n",
        "\n",
        "[2] Zhizheng Wu, Tomi Kinnunen, Nicholas Evans, Junichi Yamagishi, Cemal Hanilc, Md Sahidullah Aleksandr Sizov, \"ASVspoof 2015: the First Automatic Speaker Verification Spoofing and Countermeasures Challenge\", Proc. Interspeech 2015  2037-2041 September 2015\n",
        "\n",
        "[3] Tomi Kinnunen, Md Sahidullah, Héctor Delgado, Massimiliano Todisco, Nicholas Evans, Junichi Yamagishi, Kong Aik Lee, \"The ASVspoof 2017 Challenge: Assessing the Limits of Replay Spoofing Attack Detection\", Proc. Interspeech 2017, August 2017\n",
        "\n",
        "[4] T. Kinnunen, K. Lee, H. Delgado, N. Evans, M. Todisco, M. Sahidullah, J. Yamagishi, and D. A. Reynolds, “t-DCF: a detection cost function for the tandem assessment of spoofing countermeasures and automatic speaker verification,” in Proc. Odyssey, June 2018.\n",
        "\n",
        "[5] VCTK corpus: http://dx.doi.org/10.7488/ds/1994"
      ]
    }
  ]
}